{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672de1e2-f1c0-4013-999e-38c4f3510711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout, Flatten, \n",
    "                                    Dense, Reshape, Bidirectional, LSTM, \n",
    "                                    InputLayer, BatchNormalization, Activation,\n",
    "                                    Add, Lambda, Input, Concatenate)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0068244-53bc-4d2c-834e-6922d5a57e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Self-attention mechanism for driver-specific features\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]),\n",
    "                                initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],),\n",
    "                                initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        et = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        at = tf.nn.softmax(et, axis=1)\n",
    "        output = x * at\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b026c-bee7-4534-bc41-f48ffd8f8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Frequency-wise attention for track features\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FrequencyAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.conv1 = Conv2D(1, (1, 1), activation='sigmoid')\n",
    "        super(FrequencyAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        attention = self.conv1(x)\n",
    "        return x * attention\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532edd0-486b-4681-853b-2b29ece2e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_csv_reader(csv_path):\n",
    "    \"\"\"Handle problematic CSV files with manual parsing\"\"\"\n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  # Read header\n",
    "        for row in reader:\n",
    "            if len(row) != len(header):\n",
    "                # Handle rows with incorrect field count\n",
    "                row = [field.strip() for field in ','.join(row).split(',')[:len(header)]]\n",
    "            data.append(row)\n",
    "    return pd.DataFrame(data, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f5951-2794-4408-8805-1b24b2726db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path, sr=22050):\n",
    "    \"\"\"Simplified feature extraction with error handling\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "        features = {\n",
    "            'mfcc': librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20),\n",
    "            'mel': librosa.power_to_db(librosa.feature.melspectrogram(y=y, sr=sr)),\n",
    "            'chroma': librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        }\n",
    "        # Standardize feature lengths\n",
    "        return {k: librosa.util.fix_length(v, size=128, axis=1).T for k, v in features.items()}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49080111-8c49-45a4-89b6-67757912d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path):\n",
    "    \"\"\"Load dataset with robust CSV handling and progress tracking\"\"\"\n",
    "    try:\n",
    "        # Try multiple CSV reading methods\n",
    "        try:\n",
    "            print(\"Attempting to read CSV with pandas...\")\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as pd_error:\n",
    "            print(f\"Pandas read failed ({str(pd_error)}), trying manual parser...\")\n",
    "            df = robust_csv_reader(csv_path)\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "        required_cols = ['team_name', 'driver_name', 'track_name', 'file_path']\n",
    "        \n",
    "        # Validate columns\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        \n",
    "        print(f\"\\nFound {len(df)} entries in CSV. Starting processing...\")\n",
    "        \n",
    "        # Process files\n",
    "        results = {'X_team': [], 'X_driver': [], 'X_track': [],\n",
    "                   'y_team': [], 'y_driver': [], 'y_track': []}\n",
    "        processed_files = 0\n",
    "        skipped_files = 0\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            file_path = row['file_path']\n",
    "            if not isinstance(file_path, str):\n",
    "                print(f\"Row {idx+1}: Invalid file path (not string)\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "                \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Row {idx+1}: File not found - {file_path}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {idx+1}/{len(df)}: {os.path.basename(file_path)}\")\n",
    "            features = extract_features(file_path)\n",
    "            \n",
    "            if not features:\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "                \n",
    "            results['X_team'].append(features['mfcc'])\n",
    "            results['X_driver'].append(features['chroma'])\n",
    "            results['X_track'].append(features['mel'])\n",
    "            results['y_team'].append(row['team_name'])\n",
    "            results['y_driver'].append(row['driver_name'])\n",
    "            results['y_track'].append(row['track_name'])\n",
    "            processed_files += 1\n",
    "        \n",
    "        # Conversion and validation\n",
    "        for key in results:\n",
    "            if key.startswith('X_'):\n",
    "                results[key] = np.array(results[key])\n",
    "                if results[key].ndim == 3:\n",
    "                    results[key] = np.expand_dims(results[key], -1)\n",
    "        \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Successfully processed: {processed_files} files\")\n",
    "        print(f\"Skipped: {skipped_files} files\")\n",
    "        \n",
    "        if len(results['X_team']) == 0:\n",
    "            raise ValueError(\"No valid audio files processed\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nDataset loading failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc2d6f-ef9a-46df-a9a9-3ca4a0de5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Enhanced CNN-BiLSTM model architecture with task-specific improvements\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Common initial layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # Task-specific architectures\n",
    "    if num_classes <= 10:  # Team model (assuming <=10 teams)\n",
    "        # Enhanced team model with residual connections\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Residual block\n",
    "        residual = x\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.add([residual, x])\n",
    "        \n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "    elif num_classes <= 30:  # Driver model (assuming <=30 drivers)\n",
    "        # Hierarchical-aware driver model with attention\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "        x = tf.keras.layers.Multiply()([x, attention])\n",
    "        \n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "    else:  # Track model\n",
    "        # Enhanced track model with frequency attention\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Frequency attention block\n",
    "        freq_attention = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        freq_attention = Dense(input_shape[0], activation='softmax')(freq_attention)\n",
    "        freq_attention = Reshape((input_shape[0], 1, 1))(freq_attention)\n",
    "        x = tf.keras.layers.Multiply()([x, freq_attention])\n",
    "        \n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Common final layers\n",
    "    x = Reshape((-1, 128 if num_classes > 30 else 64))(x)\n",
    "    x = Bidirectional(LSTM(128 if num_classes > 10 else 64, return_sequences=True))(x)\n",
    "    \n",
    "    if num_classes <= 30:  # For team and driver models\n",
    "        x = tf.keras.layers.Attention()([x, x])\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    else:  # For track model\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = Dense(256 if num_classes > 30 else 128, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398d6f7-9e4d-48e2-b7f1-960e5b64f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, label_type, epochs=75): \n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_enc = to_categorical(le.fit_transform(y))\n",
    "    \n",
    "    # Train/Val/Test split (60/20/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_enc, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.25, random_state=42, stratify=np.argmax(y_train, axis=1))\n",
    "    \n",
    "    # Verify splits\n",
    "    print(f\"\\n{label_type.upper()} Data Split:\")\n",
    "    print(f\"Train samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(X_train.shape[1:], y_train.shape[1])\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),\n",
    "        ModelCheckpoint(f'best_{label_type}_model.keras', \n",
    "                       save_best_only=True,\n",
    "                       monitor='val_loss',\n",
    "                       mode='min'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(f\"\\nTraining {label_type} model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_pred_classes = np.argmax(test_pred, axis=1)\n",
    "    test_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Results\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(test_true_classes, test_pred_classes),\n",
    "        'f1_score': f1_score(test_true_classes, test_pred_classes, average='weighted'),\n",
    "        'report': classification_report(test_true_classes, test_pred_classes, target_names=le.classes_),\n",
    "        'model': model,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "        'history': history.history\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{label_type.upper()} Results:\")\n",
    "    print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"Test Loss: {results['test_loss']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['report'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfeb281-a4b1-49bf-9117-dae30121362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    CSV_PATH = \"/Volumes/Sandhya TB2/F1 MAIN/F1/f1_12race_dataset.csv\" # path to the csv containing lap-wise metadata\n",
    "    OUTPUT_DIR = \"/Users/govindamadhavabs/Desktop/F1_newModels\"        # path for the models and encoders to be saved\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(CSV_PATH)\n",
    "    if not dataset:\n",
    "        return\n",
    "    \n",
    "    # Initialize storage for results and histories\n",
    "    all_results = {}\n",
    "    training_histories = {}\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Train models\n",
    "    for label in ['team', 'driver', 'track']:\n",
    "        X = dataset[f'X_{label}']\n",
    "        y = dataset[f'y_{label}']\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(f\"\\nNo data available for {label}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Training {label} model\")\n",
    "        print(f\"Sample shape: {X[0].shape}\")\n",
    "        print(f\"Total samples: {len(X)}\")\n",
    "        print(f\"Classes: {len(set(y))}\")\n",
    "        \n",
    "        # Train and store results\n",
    "        results = train_model(X, y, label, epochs=75)\n",
    "        all_results[label] = results\n",
    "        training_histories[label] = results['model'].history.history\n",
    "        \n",
    "        # Save encoder\n",
    "        le = LabelEncoder()\n",
    "        le.fit(y)\n",
    "        label_encoders[label] = le\n",
    "        \n",
    "        # Save model and features\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = os.path.join(OUTPUT_DIR, f\"{label}_model_{timestamp}.keras\")\n",
    "        features_path = os.path.join(OUTPUT_DIR, f\"{label}_features_{timestamp}.npy\")\n",
    "        \n",
    "        results['model'].save(model_path)\n",
    "        np.save(features_path, X)\n",
    "        print(f\"Saved {label} model to {model_path}\")\n",
    "        print(f\"Saved {label} features to {features_path}\")\n",
    "    \n",
    "    # Save training histories and encoders\n",
    "    with open(os.path.join(OUTPUT_DIR, 'training_histories.pkl'), 'wb') as f:\n",
    "        pickle.dump(training_histories, f)\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'label_encoders.pkl'), 'wb') as f:\n",
    "        pickle.dump(label_encoders, f)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"FINAL MODEL SUMMARY\".center(50))\n",
    "    print(\"=\"*50)\n",
    "    for label, results in all_results.items():\n",
    "        print(f\"\\n{label.upper()} MODEL:\")\n",
    "        print(f\"- Test Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"- F1 Score: {results['f1_score']:.4f}\")\n",
    "        print(f\"- Test Loss: {results['test_loss']:.4f}\")\n",
    "        print(\"- Class Distribution:\")\n",
    "        print(pd.Series(dataset[f'y_{label}']).value_counts())\n",
    "    \n",
    "    print(\"\\nAll artifacts saved to:\", os.path.abspath(OUTPUT_DIR))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
